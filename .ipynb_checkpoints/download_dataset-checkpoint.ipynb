{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from pytube import YouTube\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import urllib.request\n",
    "try:\n",
    "    from urllib.request import Request, urlopen  # Python 3\n",
    "except:\n",
    "    from urllib2 import Request, urlopen \n",
    "    \n",
    "    \n",
    "    \n",
    "def download_youtube_videos(class_name, queries, main_path):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    data = None\n",
    "    queries = [q.replace(\" \", \"+\") for q in queries]\n",
    "    base_url = \"https://www.youtube.com/results?search_query=\" \n",
    "    urls = []\n",
    "    for q in queries:\n",
    "        req = urllib.request.Request(base_url + q, data, headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            the_page = response.read()\n",
    "        soup = BeautifulSoup(the_page)\n",
    "        mydivs = soup.findAll(\"div\", { \"id\" : \"results\" })[0]\n",
    "        a = mydivs.find_all(\"a\")\n",
    "        b = filter(lambda x: \"watch\" in x, map(lambda x: x.get(\"href\", -1), a))\n",
    "        urls += [\"https://www.youtube.com\"+x for x in list(b)]\n",
    "    for url in urls:\n",
    "        filename = main_path + class_name + \"/tmp_1\"\n",
    "        cmd = \"rm \" + filename\n",
    "        os.system(cmd)\n",
    "        \n",
    "        yt = YouTube(url)\n",
    "\n",
    "        try:\n",
    "            video = yt.get('mp4', '720p')\n",
    "        except:\n",
    "            video = yt.get('mp4')\n",
    "        video.download(filename)\n",
    "        \n",
    "        \n",
    "        outputfile = main_path + class_name + \"/\" + url.split(\"?v=\")[-1] + \"/\"\n",
    "        cmd = \"mkdir \" + outputfile\n",
    "        os.system(cmd)\n",
    "        cmd='ffmpeg -i '+filename+' -r 1 -s 224x224 ' + outputfile + 'f_%04d.jpg'\n",
    "        os.system(cmd)\n",
    "        sys.stdout.write(\"Video downloaded at: \"+ outputfile + \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "    \n",
    "def process_video_queries(main_path, classes_dict):\n",
    "    \n",
    "    for class_name, queries in list(classes_dict.items()):\n",
    "        cmd = \"mkdir \" + main_path + class_name + \"/\"\n",
    "        os.system(cmd)\n",
    "    \n",
    "        sys.stdout.write(\"Processing \" + class_name + \"\\n\")\n",
    "        download_youtube_videos(class_name, queries, main_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_images_queries(main_path, classes_dict):\n",
    "    for class_name, queries in list(classes_dict.items()):\n",
    "        cmd = \"mkdir \" + main_path + class_name + \"/\"\n",
    "        os.system(cmd)\n",
    "        queries = [q.replace(\" \", \"+\") for q in queries]\n",
    "        sys.stdout.write(\"Processing \" + class_name + \"\\n\")\n",
    "        download_google_pics(class_name, queries, main_path)\n",
    "        download_flickr_pics(class_name, queries, main_path)\n",
    "        \n",
    "\n",
    "def download_flickr_pics(class_name, queries, main_path):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    data = None\n",
    "    base_url = \"https://www.flickr.com/search/?orientation=landscape%2Csquare%2Cpanorama&text=\"\n",
    "    urls = []\n",
    "    outputfile = main_path + class_name + \"/\"\n",
    "    for q in queries:\n",
    "        req = urllib.request.Request(base_url + q, data, headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            the_page = response.read()\n",
    "        soup = BeautifulSoup(the_page)\n",
    "        m = list(soup.findAll(\"div\", {\"class\":\"view photo-list-view requiredToShowOnServer\"})[0])\n",
    "        urls += [re.search(\"url\\(//([\\w\\./])+\",str(t)).group().replace(\"url(//\", \"http://\") for t in m]\n",
    "    for url in urls:\n",
    "        urllib.request.urlretrieve(url, outputfile + url.split(\"/\")[-1])\n",
    "        \n",
    "\n",
    "def get_url_img(t):\n",
    "    r = re.search(\"url\\(//([\\w\\./])+\",str(t))\n",
    "    if r != None:\n",
    "        return r.group().replace(\"url(//\", \"http://\")\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def download_google_pics(class_name, queries, main_path):\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    headers = { 'User-Agent' : user_agent }\n",
    "    data = None\n",
    "    base_url = \"https://www.google.com/search?source=lnms&tbm=isch&sa=X&q=\"\n",
    "\n",
    "    urls = []\n",
    "    outputfile = main_path + class_name + \"/\"\n",
    "    for q in queries:\n",
    "        req = urllib.request.Request(base_url + q, data, headers)\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            the_page = response.read()\n",
    "        soup = BeautifulSoup(the_page)\n",
    "        m = soup.findAll(\"a\")\n",
    "        urls += [t.find(\"img\")[\"src\"] for t in m if t.find(\"img\") != None]\n",
    "    for url in urls:\n",
    "        urllib.request.urlretrieve(url, outputfile + url.split(\"?q=tbn:\")[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_d = {\"Cocacola\": [\"cocacola ads\", \"cocacola commercial\"],\n",
    "       \"Nike\": [\"nike ad\", \"nike reviews\"]\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Cocacola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Nike\n"
     ]
    }
   ],
   "source": [
    "process_images_queries( \"/Users/francescoferrari/Downloads/test/\", q_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process_video_queries( \"/Users/francescoferrari/Downloads/test/\", q_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
